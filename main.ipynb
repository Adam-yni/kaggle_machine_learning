{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Load GeoJSON file\n",
    "geojson_file = 'train.geojson'\n",
    "gdf = gpd.read_file(geojson_file)\n",
    "\n",
    "\n",
    "# Save DataFrame to XLSX\n",
    "xlsx = 'train.xlsx'\n",
    "gdf.to_excel(xlsx, index=False)\n",
    "\n",
    "# Load GeoJSON file\n",
    "geojson_file = 'test.geojson'\n",
    "gdf = gpd.read_file(geojson_file)\n",
    "\n",
    "\n",
    "# Save DataFrame to XLSX\n",
    "xlsx = 'test.xlsx'\n",
    "gdf.to_excel(xlsx, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import import_ipynb\n",
    "import processing_data\n",
    "import pandas as pd\n",
    "df_test = pd.read_excel('test.xlsx')\n",
    "processing_data.processed_file(df_test,'test_processed')\n",
    "\n",
    "df_train = pd.read_excel('train.xlsx')\n",
    "processing_data.processed_file(df_train,'train_processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there, I used models that I thought were good based on the f1 score metric I was using and some grid research. Using these models, I predicted the labels I needed, and in the end I came 6th out of over 130 competitors in the kaggle competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Load all your data from the Excel file\n",
    "X = pd.read_excel('train_processed.xlsx')  # Read training data from Excel\n",
    "\n",
    "# Separate features and the target variable\n",
    "X_all = X.drop(columns=['change_type'])  # Drop the target column from features\n",
    "y_all = X['change_type']  # Extract the target column\n",
    "\n",
    "# Encode class names into numerical values\n",
    "label_encoder = LabelEncoder()  # Initialize the label encoder\n",
    "y_all_encoded = label_encoder.fit_transform(y_all)  # Fit and transform target variable\n",
    "\n",
    "# Create the XGBoost dataset with all the data\n",
    "dall = xgb.DMatrix(X_all, label=y_all_encoded)  # Create XGBoost data matrix\n",
    "\n",
    "# Define model parameters\n",
    "params = {\n",
    "    'objective': 'multi:softmax',  # Multi-class classification\n",
    "    'num_class': len(set(y_all_encoded)),  # Number of classes\n",
    "    'eval_metric': 'mlogloss',  # Evaluation metric for multi-class log-loss\n",
    "    'max_depth': 8,  # Maximum depth of trees\n",
    "    'learning_rate': 0.24  # Learning rate\n",
    "}\n",
    "\n",
    "# Train the model with 1695 trees on all the data\n",
    "num_rounds = 1695  # Number of boosting rounds\n",
    "bst = xgb.train(params, dall, num_rounds)  # Train the XGBoost model\n",
    "\n",
    "# Load new data for which you want to make predictions\n",
    "new_data = pd.read_excel('test_processed.xlsx')  # Load test data from Excel\n",
    "\n",
    "# Preprocess the new data in the same way as the old data\n",
    "X_new = new_data  # Assign new data to variable X_new\n",
    "# No need to transform y_new, as it's what we want to predict\n",
    "\n",
    "# Create an XGBoost dataset for the new data\n",
    "dnew = xgb.DMatrix(X_new)  # Create data matrix for the new data\n",
    "\n",
    "# Make predictions on the new data\n",
    "y_pred_encoded = bst.predict(dnew)  # Get the encoded predictions\n",
    "\n",
    "# Convert the encoded predictions back to the original class names\n",
    "y_pred = label_encoder.inverse_transform(y_pred_encoded.astype(int))  # Convert to class names\n",
    "\n",
    "# Display the predictions\n",
    "print(\"Predictions on the new data:\")  # Print the results\n",
    "print(y_pred)  # Display the predictions\n",
    "\n",
    "# Dictionary for mapping original class names to specific values\n",
    "correspondance = {\n",
    "    'Demolition': 0,\n",
    "    'Road': 1,\n",
    "    'Residential': 2,\n",
    "    'Commercial': 3,\n",
    "    'Industrial': 4,\n",
    "    'Mega Projects': 5\n",
    "}\n",
    "\n",
    "# Function to map values based on the dictionary\n",
    "def mapper(valeur):\n",
    "    return correspondance.get(valeur, valeur)  # Return mapped value or the original value if not in dictionary\n",
    "\n",
    "# Apply the mapping function to the predictions array\n",
    "y_pred = np.vectorize(mapper)(y_pred)  # Vectorize the mapping function and apply it\n",
    "\n",
    "# Save results to a submission file\n",
    "pred_df = pd.DataFrame(y_pred)  # Create a DataFrame with predictions\n",
    "pred_df.to_csv(\"sample_submission1.csv\", index=True, index_label='Id')  # Save to CSV with index labeled as 'Id'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Load the data from an Excel file\n",
    "data = pd.read_excel('train_processed.xlsx')  # Load the training data\n",
    "\n",
    "# Separate features and the target variable\n",
    "X = data.drop(columns=['change_type'])  # Extract the features\n",
    "y = data['change_type']  # Extract the target variable\n",
    "\n",
    "# Encode the target variable as numeric values\n",
    "label_encoder = LabelEncoder()  # Initialize the label encoder\n",
    "y = label_encoder.fit_transform(y)  # Transform the target variable\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)  # 10% test set\n",
    "\n",
    "# Create different XGBoost models\n",
    "model1 = XGBClassifier(max_depth=8, n_estimators=1700, learning_rate=0.25)  # Model 1 with 1700 trees\n",
    "model2 = XGBClassifier(max_depth=8, n_estimators=1695, learning_rate=0.24)  # Model 2 with 1695 trees\n",
    "model3 = XGBClassifier(max_depth=8, n_estimators=1705, learning_rate=0.25)  # Model 3 with 1705 trees\n",
    "\n",
    "# Create a VotingClassifier with the XGBoost models\n",
    "voting_clf = VotingClassifier(estimators=[('model1', model1), ('model2', model2), ('model3', model3)], voting='soft')  # Soft voting\n",
    "\n",
    "# Train the VotingClassifier\n",
    "voting_clf.fit(X, y)  # Train on the entire dataset\n",
    "\n",
    "# Load new data for which you want to make predictions\n",
    "new_data = pd.read_excel('test_processed.xlsx')  # Load test data from Excel\n",
    "\n",
    "# Preprocess the new data the same way as the old data\n",
    "X_new = new_data  # Assign the new data to variable X_new\n",
    "# No need to transform y_new, as that's what we're predicting\n",
    "\n",
    "# Make predictions on the new data with the VotingClassifier\n",
    "y_pred_encoded = voting_clf.predict(X_new)  # Get the predictions\n",
    "\n",
    "# Convert the encoded predictions back to the original class names\n",
    "y_pred = label_encoder.inverse_transform(y_pred_encoded.astype(int))  # Convert to class names\n",
    "\n",
    "# Display the predictions\n",
    "print(\"Predictions on the new data:\")  # Output to console\n",
    "print(y_pred)  # Print the predictions\n",
    "\n",
    "# Dictionary for mapping class names to specific values\n",
    "correspondance = {\n",
    "    'Demolition': 0,\n",
    "    'Road': 1,\n",
    "    'Residential': 2,\n",
    "    'Commercial': 3,\n",
    "    'Industrial': 4,\n",
    "    'Mega Projects': 5\n",
    "}\n",
    "\n",
    "# Function to map values based on the dictionary\n",
    "def mapper(valeur):\n",
    "    return correspondance.get(valeur, valeur)  # Return the mapped value or the original if not found\n",
    "\n",
    "# Apply the transformation to the predictions array\n",
    "y_pred = np.vectorize(mapper)(y_pred)  # Vectorize the mapper function and apply\n",
    "\n",
    "# Save results to a submission file\n",
    "pred_df = pd.DataFrame(y_pred)  # Create a DataFrame for the predictions\n",
    "pred_df.to_csv(\"sample_submission_voting2.csv\", index=True, index_label='Id')  # Save to CSV with index labeled as 'Id'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the data from an Excel file\n",
    "data = pd.read_excel('train_processed.xlsx')  # Read the dataset from Excel\n",
    "\n",
    "# Separate features and the target variable\n",
    "X = data.drop(columns=['change_type'])  # Drop the target column to get the features\n",
    "y = data['change_type']  # Extract the target column\n",
    "\n",
    "# Encode class names into numerical values\n",
    "label_encoder = LabelEncoder()  # Initialize the label encoder\n",
    "y = label_encoder.fit_transform(y)  # Transform class names into numeric labels\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)  # 90% training, 10% test\n",
    "\n",
    "# Define parameter grid for the grid search\n",
    "param_grid = {\n",
    "    'learning_rate': [0.25, 0.24, 0.26],  # Learning rates to test\n",
    "    'max_depth': [8],  # Fixed depth of 8\n",
    "    'n_estimators': [1700, 1695],  # Number of trees to test\n",
    "    'gamma': [0],  # No gamma\n",
    "    'alpha': [0],  # No alpha\n",
    "    'lambda': [0]  # No lambda\n",
    "}\n",
    "\n",
    "# Create the XGBoost classifier\n",
    "xgb_classifier = xgb.XGBClassifier(objective='multi:softmax', num_class=len(set(y)), seed=42)  # Multi-class with fixed seed\n",
    "\n",
    "# Grid search to find the best parameters\n",
    "grid_search = GridSearchCV(estimator=xgb_classifier, param_grid=param_grid, cv=3, scoring='f1_macro')  # 3-fold cross-validation\n",
    "grid_search.fit(X_train, y_train)  # Fit the grid search on the training data\n",
    "\n",
    "# Display the best parameters found\n",
    "print(\"Best parameters found:\")  # Print best parameters\n",
    "print(grid_search.best_params_)  # Show the best parameters found by grid search\n",
    "\n",
    "# Predictions on the test set with the best parameters\n",
    "best_xgb_classifier = grid_search.best_estimator_  # Get the best classifier\n",
    "y_pred = best_xgb_classifier.predict(X_test)  # Make predictions on the test set\n",
    "\n",
    "# Calculate F1-score for model accuracy\n",
    "f1 = f1_score(y_test, y_pred, average='micro')  # Micro-average F1-score\n",
    "print(\"Model accuracy with best parameters:\", f1)  # Display model accuracy\n",
    "\n",
    "# Display the classification report\n",
    "print(\"Classification report:\")  # Output the classification report\n",
    "print(classification_report(y_test, y_pred))  # Show the detailed report\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
