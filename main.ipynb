{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Load GeoJSON file\n",
    "geojson_file = 'train.geojson'\n",
    "gdf = gpd.read_file(geojson_file)\n",
    "\n",
    "\n",
    "# Save DataFrame to XLSX\n",
    "xlsx = 'train.xlsx'\n",
    "gdf.to_excel(xlsx, index=False)\n",
    "\n",
    "# Load GeoJSON file\n",
    "geojson_file = 'test.geojson'\n",
    "gdf = gpd.read_file(geojson_file)\n",
    "\n",
    "\n",
    "# Save DataFrame to XLSX\n",
    "xlsx = 'test.xlsx'\n",
    "gdf.to_excel(xlsx, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#PREPROCESSING TEST PART\n",
    "# Charger le fichier Excel\n",
    "chemin_fichier_excel = 'test.xlsx'\n",
    "df = pd.read_excel(chemin_fichier_excel)\n",
    "df.iloc[:, 0] = df.iloc[:, 0].apply(lambda x: x.replace(' ', '') if isinstance(x, str) else x)\n",
    "df.iloc[:, 1] = df.iloc[:, 1].apply(lambda x: x.replace(' ', '') if isinstance(x, str) else x)\n",
    "colonnes_a_traiter = df.columns[2:32] \n",
    "\n",
    "# Calculer la médiane pour chaque colonne spécifiée\n",
    "medianes = df[colonnes_a_traiter].median()\n",
    "\n",
    "# Remplacer les cellules vides par les médianes correspondantes\n",
    "df[colonnes_a_traiter] = df[colonnes_a_traiter].fillna(medianes)\n",
    "def processv2(df):\n",
    "    data = df['geometry']\n",
    "    liste=[]\n",
    "    for ligne in data:\n",
    "        sl=[]\n",
    "        ligne=ligne[10:-2]\n",
    "        polygone = ligne.split(',')\n",
    "        for element in polygone:\n",
    "            a,b= [i for i in element.split(' ') if i != '']\n",
    "            sl.append((float(a),float(b)))\n",
    "        liste.append(sl)\n",
    "    return liste\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "def calculer_aire_polygone(sommets):\n",
    "    \"\"\"\n",
    "    Calcule l'aire d'un polygone à l'aide de la bibliothèque shapely.\n",
    "    \n",
    "    :param sommets: Liste des sommets du polygone sous forme de tuples (x, y)\n",
    "    :return: L'aire du polygone\n",
    "    \"\"\"\n",
    "\n",
    "    polygone = Polygon(sommets)\n",
    "    aire = polygone.area\n",
    "    return aire\n",
    "def perim(sommets):\n",
    "    polygone = Polygon(sommets)\n",
    "# Calculer le périmètre du polygone\n",
    "    perimetre = polygone.length\n",
    "    return perimetre\n",
    "def calculate_height_width_ratio(sommets):\n",
    "    polygon = Polygon(sommets)\n",
    "    bounds = polygon.bounds\n",
    "    height = bounds[3] - bounds[1]  # ymax - ymin\n",
    "    width = bounds[2] - bounds[0]   # xmax - xmin\n",
    "    return height / width\n",
    "\n",
    "def calculate_centroid_x(sommets):\n",
    "    polygon = Polygon(sommets)\n",
    "    centroid = polygon.centroid\n",
    "    return centroid.x\n",
    "\n",
    "def calculate_centroid_y(sommets):\n",
    "    polygon = Polygon(sommets)\n",
    "    centroid = polygon.centroid\n",
    "    return centroid.y\n",
    "# Calculer l'aire de chaque polygone dans la liste\n",
    "liste_aires = [calculer_aire_polygone(polygone) for polygone in processv2(df)]\n",
    "liste_perim = [perim(polygone) for polygone in processv2(df)]\n",
    "liste_ratio = [calculate_height_width_ratio(polygone) for polygone in processv2(df)]\n",
    "liste_centroid_x = [calculate_centroid_x(polygone) for polygone in processv2(df)]\n",
    "liste_centroid_y = [calculate_centroid_y(polygone) for polygone in processv2(df)]\n",
    "# Ajouter la nouvelle colonne au DataFrame\n",
    "df['Area'] = liste_aires\n",
    "df['Perimeter']=liste_perim\n",
    "df['ratio']=liste_ratio\n",
    "df['x_centroid']=liste_centroid_x\n",
    "df['y_centroid']=liste_centroid_y\n",
    "df = df.drop(['geometry'],axis=1)\n",
    "df.iloc[:, 0] = df.iloc[:, 0].str.replace('N,A', 'NSP_urban')\n",
    "\n",
    "df.iloc[:, 1] = df.iloc[:, 1].str.replace('N,A', 'NSP_geo')\n",
    "one_hot_encoded = df['urban_type'].str.get_dummies(sep=',')\n",
    "\n",
    "df = df.join(one_hot_encoded)\n",
    "\n",
    "one_hot_encoded = df['geography_type'].str.get_dummies(sep=',')\n",
    "\n",
    "df = df.join(one_hot_encoded)\n",
    "\n",
    "\n",
    "colonnes_a_supprimer = ['urban_type', 'geography_type','index']  # Remplacez par les noms des colonnes à supprimer\n",
    "df = df.drop(columns=colonnes_a_supprimer)\n",
    "\n",
    "df['date0'] = pd.to_datetime(df['date0'], format='%d-%m-%Y')\n",
    "df['date1'] = pd.to_datetime(df['date1'], format='%d-%m-%Y')\n",
    "df['date2'] = pd.to_datetime(df['date2'], format='%d-%m-%Y')\n",
    "df['date3'] = pd.to_datetime(df['date3'], format='%d-%m-%Y')\n",
    "df['date4'] = pd.to_datetime(df['date4'], format='%d-%m-%Y')\n",
    "\n",
    "# Calculer l'écart en nombre de jours entre les deux dates\n",
    "df['Ecart_jours1'] = (df['date1'] - df['date0']).dt.days\n",
    "df['Ecart_jours2'] = (df['date2'] - df['date1']).dt.days\n",
    "df['Ecart_jours3'] = (df['date3'] - df['date2']).dt.days\n",
    "df['Ecart_jours4'] = (df['date4'] - df['date3']).dt.days\n",
    "\n",
    "colonnes_a_supprimer = ['date0','date1','date2','date3','date4']  \n",
    "df = df.drop(columns=colonnes_a_supprimer)\n",
    "\n",
    "# Appliquer le One-Hot Encoding sur la colonne 'Categories'\n",
    "one_hot_encoded0 = df['change_status_date0'].str.get_dummies(sep=',')\n",
    "one_hot_encoded1 = df['change_status_date1'].str.get_dummies(sep=',')\n",
    "one_hot_encoded2 = df['change_status_date2'].str.get_dummies(sep=',')\n",
    "one_hot_encoded3 = df['change_status_date3'].str.get_dummies(sep=',')\n",
    "one_hot_encoded4 = df['change_status_date4'].str.get_dummies(sep=',')\n",
    "\n",
    "df = df.join(one_hot_encoded0,rsuffix='0')\n",
    "df = df.join(one_hot_encoded1,rsuffix='1')\n",
    "df = df.join(one_hot_encoded2,rsuffix='2')\n",
    "df = df.join(one_hot_encoded3,rsuffix='3')\n",
    "df = df.join(one_hot_encoded4,rsuffix='4')\n",
    "\n",
    "colonnes_a_traiter=['Ecart_jours1','Ecart_jours2','Ecart_jours3','Ecart_jours4']\n",
    "medianes = df[colonnes_a_traiter].median()\n",
    "\n",
    "# Remplacer les cellules vides par les médianes correspondantes\n",
    "df[colonnes_a_traiter] = df[colonnes_a_traiter].fillna(medianes)\n",
    "colonnes_a_supprimer = ['change_status_date0','change_status_date1','change_status_date2','change_status_date3','change_status_date4'] \n",
    "df = df.drop(columns=colonnes_a_supprimer)\n",
    "couleurs = ['red', 'blue', 'green']\n",
    "\n",
    "# Créer une liste des dates à considérer (date1, date2, date3, date4, date5)\n",
    "dates = ['date1', 'date2', 'date3', 'date4', 'date5']\n",
    "\n",
    "# Parcourir chaque couleur\n",
    "for couleur in couleurs:\n",
    "    # Créer une liste pour stocker les moyennes de chaque ligne\n",
    "    moyennes = []\n",
    "    # Parcourir chaque ligne\n",
    "    for index, row in df.iterrows():\n",
    "        # Créer une liste pour stocker les valeurs de chaque date pour cette couleur\n",
    "        valeurs_couleur = []\n",
    "        # Parcourir chaque date\n",
    "        for date in dates:\n",
    "            # Récupérer le nom de la colonne\n",
    "            nom_colonne = f'img_{couleur}_mean_{date}'\n",
    "            # Ajouter la valeur de la colonne à la liste des valeurs de cette couleur\n",
    "            valeurs_couleur.append(row[nom_colonne])\n",
    "        # Calculer la moyenne des valeurs pour cette ligne\n",
    "        moyenne_ligne = sum(valeurs_couleur) / len(valeurs_couleur)\n",
    "        # Ajouter la moyenne à la liste des moyennes\n",
    "        moyennes.append(moyenne_ligne)\n",
    "    # Créer une nouvelle colonne dans le DataFrame pour stocker les moyennes de cette couleur\n",
    "    nom_nouvelle_colonne = f'moyenne_{couleur}'\n",
    "    df[nom_nouvelle_colonne] = moyennes\n",
    "for couleur in couleurs:\n",
    "    # Créer une liste pour stocker les moyennes de chaque ligne\n",
    "    moyennes = []\n",
    "    # Parcourir chaque ligne\n",
    "    for index, row in df.iterrows():\n",
    "        # Créer une liste pour stocker les valeurs de chaque date pour cette couleur\n",
    "        valeurs_couleur = []\n",
    "        # Parcourir chaque date\n",
    "        for date in dates:\n",
    "            # Récupérer le nom de la colonne\n",
    "            nom_colonne = f'img_{couleur}_std_{date}'\n",
    "            # Ajouter la valeur de la colonne à la liste des valeurs de cette couleur\n",
    "            valeurs_couleur.append(row[nom_colonne])\n",
    "        # Calculer la moyenne des valeurs pour cette ligne\n",
    "        moyenne_ligne = sum(valeurs_couleur) / len(valeurs_couleur)\n",
    "        # Ajouter la moyenne à la liste des moyennes\n",
    "        moyennes.append(moyenne_ligne)\n",
    "    # Créer une nouvelle colonne dans le DataFrame pour stocker les moyennes de cette couleur\n",
    "    nom_nouvelle_colonne = f'std_{couleur}'\n",
    "    df[nom_nouvelle_colonne] = moyennes\n",
    "df.to_excel('test_final5.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#PREPROCESSING TRAIN PART\n",
    "# Charger le fichier Excel\n",
    "chemin_fichier_excel = 'train.xlsx'\n",
    "df = pd.read_excel(chemin_fichier_excel)\n",
    "df.iloc[:, 0] = df.iloc[:, 0].apply(lambda x: x.replace(' ', '') if isinstance(x, str) else x)\n",
    "df.iloc[:, 1] = df.iloc[:, 1].apply(lambda x: x.replace(' ', '') if isinstance(x, str) else x)\n",
    "colonnes_a_traiter = df.columns[3:33] \n",
    "\n",
    "# Calculer la médiane pour chaque colonne spécifiée\n",
    "medianes = df[colonnes_a_traiter].median()\n",
    "\n",
    "# Remplacer les cellules vides par les médianes correspondantes\n",
    "df[colonnes_a_traiter] = df[colonnes_a_traiter].fillna(medianes)\n",
    "def processv2(df):\n",
    "    data = df['geometry']\n",
    "    liste=[]\n",
    "    for ligne in data:\n",
    "        sl=[]\n",
    "        ligne=ligne[10:-2]\n",
    "        polygone = ligne.split(',')\n",
    "        for element in polygone:\n",
    "            a,b= [i for i in element.split(' ') if i != '']\n",
    "            sl.append((float(a),float(b)))\n",
    "        liste.append(sl)\n",
    "    return liste\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "def calculer_aire_polygone(sommets):\n",
    "    \"\"\"\n",
    "    Calcule l'aire d'un polygone à l'aide de la bibliothèque shapely.\n",
    "    \n",
    "    :param sommets: Liste des sommets du polygone sous forme de tuples (x, y)\n",
    "    :return: L'aire du polygone\n",
    "    \"\"\"\n",
    "\n",
    "    polygone = Polygon(sommets)\n",
    "    aire = polygone.area\n",
    "    return aire\n",
    "def perim(sommets):\n",
    "    polygone = Polygon(sommets)\n",
    "# Calculer le périmètre du polygone\n",
    "    perimetre = polygone.length\n",
    "    return perimetre\n",
    "def calculate_height_width_ratio(sommets):\n",
    "    polygon = Polygon(sommets)\n",
    "    bounds = polygon.bounds\n",
    "    height = bounds[3] - bounds[1]  # ymax - ymin\n",
    "    width = bounds[2] - bounds[0]   # xmax - xmin\n",
    "    return height / width\n",
    "\n",
    "def calculate_centroid_x(sommets):\n",
    "    polygon = Polygon(sommets)\n",
    "    centroid = polygon.centroid\n",
    "    return centroid.x\n",
    "\n",
    "def calculate_centroid_y(sommets):\n",
    "    polygon = Polygon(sommets)\n",
    "    centroid = polygon.centroid\n",
    "    return centroid.y\n",
    "# Calculer l'aire de chaque polygone dans la liste\n",
    "liste_aires = [calculer_aire_polygone(polygone) for polygone in processv2(df)]\n",
    "liste_perim = [perim(polygone) for polygone in processv2(df)]\n",
    "liste_ratio = [calculate_height_width_ratio(polygone) for polygone in processv2(df)]\n",
    "liste_centroid_x = [calculate_centroid_x(polygone) for polygone in processv2(df)]\n",
    "liste_centroid_y = [calculate_centroid_y(polygone) for polygone in processv2(df)]\n",
    "# Ajouter la nouvelle colonne au DataFrame\n",
    "df['Area'] = liste_aires\n",
    "df['Perimeter']=liste_perim\n",
    "df['ratio']=liste_ratio\n",
    "df['x_centroid']=liste_centroid_x\n",
    "df['y_centroid']=liste_centroid_y\n",
    "df = df.drop(['geometry'],axis=1)\n",
    "df.iloc[:, 0] = df.iloc[:, 0].str.replace('N,A', 'NSP_urban')\n",
    "\n",
    "df.iloc[:, 1] = df.iloc[:, 1].str.replace('N,A', 'NSP_geo')\n",
    "one_hot_encoded = df['urban_type'].str.get_dummies(sep=',')\n",
    "\n",
    "df = df.join(one_hot_encoded)\n",
    "\n",
    "one_hot_encoded = df['geography_type'].str.get_dummies(sep=',')\n",
    "\n",
    "df = df.join(one_hot_encoded)\n",
    "\n",
    "\n",
    "colonnes_a_supprimer = ['urban_type', 'geography_type','index']  # Remplacez par les noms des colonnes à supprimer\n",
    "df = df.drop(columns=colonnes_a_supprimer)\n",
    "\n",
    "df['date0'] = pd.to_datetime(df['date0'], format='%d-%m-%Y')\n",
    "df['date1'] = pd.to_datetime(df['date1'], format='%d-%m-%Y')\n",
    "df['date2'] = pd.to_datetime(df['date2'], format='%d-%m-%Y')\n",
    "df['date3'] = pd.to_datetime(df['date3'], format='%d-%m-%Y')\n",
    "df['date4'] = pd.to_datetime(df['date4'], format='%d-%m-%Y')\n",
    "\n",
    "# Calculer l'écart en nombre de jours entre les deux dates\n",
    "df['Ecart_jours1'] = (df['date1'] - df['date0']).dt.days\n",
    "df['Ecart_jours2'] = (df['date2'] - df['date1']).dt.days\n",
    "df['Ecart_jours3'] = (df['date3'] - df['date2']).dt.days\n",
    "df['Ecart_jours4'] = (df['date4'] - df['date3']).dt.days\n",
    "\n",
    "colonnes_a_supprimer = ['date0','date1','date2','date3','date4']  \n",
    "df = df.drop(columns=colonnes_a_supprimer)\n",
    "\n",
    "# Appliquer le One-Hot Encoding sur la colonne 'Categories'\n",
    "one_hot_encoded0 = df['change_status_date0'].str.get_dummies(sep=',')\n",
    "one_hot_encoded1 = df['change_status_date1'].str.get_dummies(sep=',')\n",
    "one_hot_encoded2 = df['change_status_date2'].str.get_dummies(sep=',')\n",
    "one_hot_encoded3 = df['change_status_date3'].str.get_dummies(sep=',')\n",
    "one_hot_encoded4 = df['change_status_date4'].str.get_dummies(sep=',')\n",
    "\n",
    "df = df.join(one_hot_encoded0,rsuffix='0')\n",
    "df = df.join(one_hot_encoded1,rsuffix='1')\n",
    "df = df.join(one_hot_encoded2,rsuffix='2')\n",
    "df = df.join(one_hot_encoded3,rsuffix='3')\n",
    "df = df.join(one_hot_encoded4,rsuffix='4')\n",
    "\n",
    "colonnes_a_traiter=['Ecart_jours1','Ecart_jours2','Ecart_jours3','Ecart_jours4']\n",
    "medianes = df[colonnes_a_traiter].median()\n",
    "\n",
    "# Remplacer les cellules vides par les médianes correspondantes\n",
    "df[colonnes_a_traiter] = df[colonnes_a_traiter].fillna(medianes)\n",
    "colonnes_a_supprimer = ['change_status_date0','change_status_date1','change_status_date2','change_status_date3','change_status_date4'] \n",
    "df = df.drop(columns=colonnes_a_supprimer)\n",
    "couleurs = ['red', 'blue', 'green']\n",
    "\n",
    "# Créer une liste des dates à considérer (date1, date2, date3, date4, date5)\n",
    "dates = ['date1', 'date2', 'date3', 'date4', 'date5']\n",
    "\n",
    "# Parcourir chaque couleur\n",
    "for couleur in couleurs:\n",
    "    # Créer une liste pour stocker les moyennes de chaque ligne\n",
    "    moyennes = []\n",
    "    # Parcourir chaque ligne\n",
    "    for index, row in df.iterrows():\n",
    "        # Créer une liste pour stocker les valeurs de chaque date pour cette couleur\n",
    "        valeurs_couleur = []\n",
    "        # Parcourir chaque date\n",
    "        for date in dates:\n",
    "            # Récupérer le nom de la colonne\n",
    "            nom_colonne = f'img_{couleur}_mean_{date}'\n",
    "            # Ajouter la valeur de la colonne à la liste des valeurs de cette couleur\n",
    "            valeurs_couleur.append(row[nom_colonne])\n",
    "        # Calculer la moyenne des valeurs pour cette ligne\n",
    "        moyenne_ligne = sum(valeurs_couleur) / len(valeurs_couleur)\n",
    "        # Ajouter la moyenne à la liste des moyennes\n",
    "        moyennes.append(moyenne_ligne)\n",
    "    # Créer une nouvelle colonne dans le DataFrame pour stocker les moyennes de cette couleur\n",
    "    nom_nouvelle_colonne = f'moyenne_{couleur}'\n",
    "    df[nom_nouvelle_colonne] = moyennes\n",
    "for couleur in couleurs:\n",
    "    # Créer une liste pour stocker les moyennes de chaque ligne\n",
    "    moyennes = []\n",
    "    # Parcourir chaque ligne\n",
    "    for index, row in df.iterrows():\n",
    "        # Créer une liste pour stocker les valeurs de chaque date pour cette couleur\n",
    "        valeurs_couleur = []\n",
    "        # Parcourir chaque date\n",
    "        for date in dates:\n",
    "            # Récupérer le nom de la colonne\n",
    "            nom_colonne = f'img_{couleur}_std_{date}'\n",
    "            # Ajouter la valeur de la colonne à la liste des valeurs de cette couleur\n",
    "            valeurs_couleur.append(row[nom_colonne])\n",
    "        # Calculer la moyenne des valeurs pour cette ligne\n",
    "        moyenne_ligne = sum(valeurs_couleur) / len(valeurs_couleur)\n",
    "        # Ajouter la moyenne à la liste des moyennes\n",
    "        moyennes.append(moyenne_ligne)\n",
    "    # Créer une nouvelle colonne dans le DataFrame pour stocker les moyennes de cette couleur\n",
    "    nom_nouvelle_colonne = f'std_{couleur}'\n",
    "    df[nom_nouvelle_colonne] = moyennes\n",
    "df.to_excel('test_final5.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Charger toutes vos données à partir du fichier Excel\n",
    "X = pd.read_excel('train_final5.xlsx')\n",
    "\n",
    "# Séparation des features et de la variable cible\n",
    "X_all = X.drop(columns=['change_type'])\n",
    "y_all = X['change_type']\n",
    "\n",
    "# Encoder les noms de classe en valeurs numériques\n",
    "label_encoder = LabelEncoder()\n",
    "y_all_encoded = label_encoder.fit_transform(y_all)\n",
    "\n",
    "# Création du dataset XGBoost avec toutes les données\n",
    "dall = xgb.DMatrix(X_all, label=y_all_encoded)\n",
    "\n",
    "# Définition des paramètres du modèle\n",
    "params = {\n",
    "    'objective': 'multi:softmax',  # Classification multi-classe\n",
    "    'num_class': len(set(y_all_encoded)),  # Nombre de classes\n",
    "    'eval_metric': 'mlogloss',  # Métrique d'évaluation pour le F1-score\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.24\n",
    "    \n",
    "}\n",
    "\n",
    "# Entraînement du modèle avec 1695 arbres sur toutes les données\n",
    "num_rounds = 1695\n",
    "bst = xgb.train(params, dall, num_rounds)\n",
    "\n",
    "# Charger les nouvelles données pour lesquelles vous souhaitez faire des prédictions\n",
    "new_data = pd.read_excel('test_final5.xlsx')\n",
    "\n",
    "# Prétraiter les nouvelles données de la même manière que les anciennes données\n",
    "X_new = new_data\n",
    "# Pas besoin de transformer y_new car c'est ce que nous voulons prédire\n",
    "\n",
    "# Créer un dataset XGBoost pour les nouvelles données\n",
    "dnew = xgb.DMatrix(X_new)\n",
    "\n",
    "# Faire des prédictions sur les nouvelles données\n",
    "y_pred_encoded = bst.predict(dnew)\n",
    "\n",
    "# Conversion des prédictions encodées en noms d'origine\n",
    "y_pred = label_encoder.inverse_transform(y_pred_encoded.astype(int))\n",
    "\n",
    "# Afficher les prédictions\n",
    "print(\"Prédictions sur les nouvelles données :\")\n",
    "print(y_pred)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Dictionnaire de correspondance\n",
    "correspondance = {\n",
    "    'Demolition': 0,\n",
    "    'Road': 1,\n",
    "    'Residential': 2,\n",
    "    'Commercial': 3,\n",
    "    'Industrial': 4,\n",
    "    'Mega Projects': 5\n",
    "}\n",
    "\n",
    "# Fonction pour mapper les valeurs\n",
    "def mapper(valeur):\n",
    "    return correspondance.get(valeur, valeur)  # Retourne la valeur correspondante ou la valeur elle-même si elle n'est pas dans le dictionnaire\n",
    "\n",
    "# Application de la transformation à l'array\n",
    "y_pred = np.vectorize(mapper)(y_pred)\n",
    "## Save results to submission file\n",
    "pred_df = pd.DataFrame(y_pred)\n",
    "pred_df.to_csv(\"sample_submissionFINALRENDU.csv\", index=True, index_label='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VOTING\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "# Charger les données à partir du fichier Excel\n",
    "data = pd.read_excel('train_final5.xlsx')\n",
    "\n",
    "# Séparation des features et de la variable cible\n",
    "X = data.drop(columns=['change_type'])\n",
    "y = data['change_type']\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "y = label_encoder.fit_transform(y)\n",
    "# Séparation des données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "# Créer différents modèles XGBoost\n",
    "model1 = XGBClassifier(max_depth=8, n_estimators=1700,learning_rate = 0.25)\n",
    "model2 = XGBClassifier(max_depth=8, n_estimators=1695,learning_rate = 0.24)\n",
    "model3 = XGBClassifier(max_depth=8, n_estimators=1705,learning_rate = 0.25)\n",
    "\n",
    "# Créer un VotingClassifier avec les modèles XGBoost\n",
    "voting_clf = VotingClassifier(estimators=[('model1', model1), ('model2', model2), ('model3', model3)], voting='soft')\n",
    "\n",
    "# Entraîner le VotingClassifier\n",
    "voting_clf.fit(X, y)\n",
    "\n",
    "# Charger les nouvelles données pour lesquelles vous souhaitez faire des prédictions\n",
    "new_data = pd.read_excel('test_final5.xlsx')\n",
    "\n",
    "# Prétraiter les nouvelles données de la même manière que les anciennes données\n",
    "X_new = new_data\n",
    "# Pas besoin de transformer y_new car c'est ce que nous voulons prédire\n",
    "\n",
    "# Créer un dataset XGBoost pour les nouvelles données\n",
    "dnew = xgb.DMatrix(X_new)\n",
    "\n",
    "# Faire des prédictions sur les nouvelles données\n",
    "y_pred_encoded = voting_clf.predict(X_new)\n",
    "\n",
    "# Conversion des prédictions encodées en noms d'origine\n",
    "y_pred = label_encoder.inverse_transform(y_pred_encoded.astype(int))\n",
    "\n",
    "# Afficher les prédictions\n",
    "print(\"Prédictions sur les nouvelles données :\")\n",
    "print(y_pred)\n",
    "import numpy as np\n",
    "\n",
    "# Dictionnaire de correspondance\n",
    "correspondance = {\n",
    "    'Demolition': 0,\n",
    "    'Road': 1,\n",
    "    'Residential': 2,\n",
    "    'Commercial': 3,\n",
    "    'Industrial': 4,\n",
    "    'Mega Projects': 5\n",
    "}\n",
    "\n",
    "# Fonction pour mapper les valeurs\n",
    "def mapper(valeur):\n",
    "    return correspondance.get(valeur, valeur)  # Retourne la valeur correspondante ou la valeur elle-même si elle n'est pas dans le dictionnaire\n",
    "\n",
    "# Application de la transformation à l'array\n",
    "y_pred = np.vectorize(mapper)(y_pred)\n",
    "## Save results to submission file\n",
    "pred_df = pd.DataFrame(y_pred)\n",
    "pred_df.to_csv(\"sample_submissionRENDUFINALVOTING.csv\", index=True, index_label='Id')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXEMPLE DE GRIDSEARCH\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Charger les données à partir du fichier Excel\n",
    "data = pd.read_excel('train_final5.xlsx')\n",
    "\n",
    "# Séparation des features et de la variable cible\n",
    "X = data.drop(columns=['change_type'])\n",
    "y = data['change_type']\n",
    "# Encoder les noms de classe en valeurs numériques\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "# Séparation des données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Définition des paramètres à tester\n",
    "param_grid = {\n",
    "    'learning_rate': [ 0.25,0.24,0.26],\n",
    "    'max_depth': [ 8],\n",
    "    'n_estimators': [1700,1695],\n",
    "    'gamma': [ 0],\n",
    "    'alpha': [0],\n",
    "    'lambda': [ 0]\n",
    "}\n",
    "   \n",
    "\n",
    "# Création du classifieur XGBoost\n",
    "xgb_classifier = xgb.XGBClassifier(objective='multi:softmax', num_class=len(set(y)) ,seed=42)\n",
    "\n",
    "# Recherche par grille pour trouver les meilleurs paramètres\n",
    "grid_search = GridSearchCV(estimator=xgb_classifier, param_grid=param_grid, cv=3, scoring='f1_macro')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Affichage des meilleurs paramètres\n",
    "print(\"Meilleurs paramètres trouvés:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Prédictions sur l'ensemble de test avec les meilleurs paramètres\n",
    "best_xgb_classifier = grid_search.best_estimator_\n",
    "y_pred = best_xgb_classifier.predict(X_test)\n",
    "\n",
    "# Calcul de la précision\n",
    "accuracy = f1_score(y_test, y_pred,average='micro')\n",
    "print(\"Précision du modèle avec les meilleurs paramètres :\", accuracy)\n",
    "\n",
    "# Affichage du rapport de classification\n",
    "print(\"Rapport de classification :\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
